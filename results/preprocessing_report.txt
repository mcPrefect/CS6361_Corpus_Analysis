
KASHUBIAN CORPUS PREPROCESSING REPORT
======================================================================

Following CS6361 Week 5 & 7 Lecture Architecture:
- Markup Analysis (XML parsing)
- Tokenization (diacritic-aware)
- Case Normalization
- Punctuation Handling
- Stopword Decisions

CORPUS STATISTICS
======================================================================

Input Processing:
  - XML elements processed: 7,428
  - Articles extracted: 6,933
  - Markup removed: 2,696,695 characters

Output Statistics:
  - Total characters (cleaned): 3,261,617
  - Total words (tokens): 498,394
  - Unique vocabulary: (calculated in frequency analysis)

PREPROCESSING DECISIONS
======================================================================

1. MARKUP ANALYSIS:
   - Removed Wikipedia templates, references, categories
   - Extracted only article namespace content
   - Excluded talk pages, user pages, meta pages

2. TOKENIZATION:
   - Custom regex for Kashubian diacritics (ą, ã, é, ë, ń, ò, ó, ô, ù, ł, ż)
   - Ensures special characters treated as valid word components
   - Pattern: [aąãbcćdeęéëfghijklłmnńoòóôprsśtuùvwyzźż]+

3. CASE NORMALIZATION:
   - All tokens converted to lowercase
   - Ensures 'Kaszëbë' and 'kaszëbë' counted as same word

4. PUNCTUATION HANDLING:
   - Removed punctuation-only tokens
   - Preserved punctuation context during initial parsing

5. STOPWORD DECISION:
   - KEPT stopwords in corpus
   - Rationale: Essential for character frequency and lexical models
   - Stopwords include: był, jego, na, się, są, ten, też, to, tylko, że ...

ALIGNMENT WITH MODULE CONTENT
======================================================================

Week 5 Lecture Concepts Applied:
✓ Markup Analysis - Wikipedia XML parsing
✓ Tokenization - Custom pattern for Kashubian diacritics
✓ Case normalization - Lowercase conversion
✓ Punctuation handling - Filtered punctuation-only tokens
✓ Stopword decisions - Kept for frequency analysis

Week 7 Lecture Concepts Applied:
✓ Language digitisation context - Kashubian ISO code: csb
✓ Unicode handling - Proper encoding for all diacritics
✓ Corpus quality - Wikipedia as authoritative source

FILES GENERATED
======================================================================

1. tokens_preprocessed.txt
   - One token per line
   - Ready for word frequency analysis

2. characters_preprocessed.txt
   - Character sequence including spaces
   - Ready for character frequency analysis

3. preprocessing_stats.json
   - Detailed statistics from preprocessing pipeline

4. article_metadata.json
   - Per-article word and character counts
   - Useful for corpus quality assessment

5. preprocessing_report.txt (this file)
   - Complete documentation of preprocessing decisions

NEXT STEPS
======================================================================

Run the frequency analysis scripts:
1. kashubian_character_frequency.py
2. kashubian_word_frequency.py
3. kashubian_zipf_analysis.py

These will generate the statistics needed for slides 5-8 of your presentation.
